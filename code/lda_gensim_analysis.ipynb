{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、预处理\n",
    "## 1. 分词以及停用词去除\n",
    "    这个步骤的主要作用是：\n",
    "```text\n",
    "1. 将所有文本进行分词形成语料库\n",
    "2. 剔除语料库中没有意义的停用词，如：我，我们，我的，他的等词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import jieba.posseg as psg\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 加载原始文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path().resolve().parent/'resource'/'data'/'raw'/'weibo_keyword.csv'\n",
    "\n",
    "# # 检测文件编码\n",
    "# import chardet\n",
    "# with open(file_path, 'rb') as f:\n",
    "#     result = chardet.detect(f.read())\n",
    "#     encoding = result['encoding']\n",
    "# print(encoding)\n",
    "\n",
    "# 只读取text列的前5000行作为实验性分析\n",
    "df = pd.read_csv(file_path, encoding='GB2312',encoding_errors='ignore', usecols=['text'], nrows=5000).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of                                                    text\n",
      "0              是哪个大学生这么倒霉哦原来是我一边浑身不舒服发烧一边还要为期末考试焦头烂额新冠你\n",
      "1     每日早报2022年07月26日星期二农历六月二十八1最高法加大对文娱领域高净值人群逃税惩处力...\n",
      "2     我的爷爷从我记事起就一直在吃药从小跟着爷爷奶奶生活的我很幸福没有受过什么委屈上学会送我放假也...\n",
      "3     252022下半年刷了一篇德国任职37岁大学教授文。大学是什么？是人类先锋某一个学生在某个时...\n",
      "4                           大学生很好年轻朝气但心智不成熟适合校园恋爱。不适合我。\n",
      "...                                                 ...\n",
      "4995  防范电信诈骗网恋女友竟是抠脚大汉日前一男子因欠下巨额债务便冒充女大学生与一名男子谈恋爱诈骗钱...\n",
      "4996  小时候的王俊凯太可爱了王俊凯在微博里说自己成绩不好因为作业极度伤心！王俊凯从小到大都是品学兼...\n",
      "4997  未来你好我刚刚在大学生最喜爱的音乐人评选活动为小鬼王琳凯投出一票！快来参加为你最爱的音乐人打...\n",
      "4998  2020中国大学生好创意为中国加油！武汉加油！全国大广赛组委会作者刘毅李竹青学校百色学院让我...\n",
      "4999  同心战疫为爱而歌2020年2月日本音乐家吉田携手湖北襄阳籍的男歌手李行亮及湖北武汉籍的演员音...\n",
      "\n",
      "[5000 rows x 1 columns]> Index(['text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 检查数据读取情况\n",
    "print(df.info,df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 读取停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取哈工大停词表，总数为 749\n",
      "成功更新停词表，此时总数为 753\n"
     ]
    }
   ],
   "source": [
    "stop_word_path = Path().resolve().parent/'resource'/'stop_words'/'hit_stopwords.txt' # 哈工大停词表\n",
    "supplement_path = Path().resolve().parent/'resource'/'stop_words'/'supplement.txt' # 根据分词结果，补充停词表\n",
    "try:\n",
    "    with open(stop_word_path, encoding='utf8') as file:\n",
    "        stop_list = {word.strip() for word in file if word.strip()}\n",
    "        print(\"成功读取哈工大停词表，总数为\", len(stop_list))\n",
    "    with open(supplement_path, encoding='utf8') as file:\n",
    "        stop_list.update({word.strip() for word in file if word.strip()})\n",
    "        print('成功更新停词表，此时总数为', len(stop_list))\n",
    "except FileNotFoundError:\n",
    "    stop_list = set()\n",
    "    print(f'Error: {stop_word_path} 没找到')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 调用jieba.posseg.cut()函数进行分词并根据词性只保留名词类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    在psg.cut(text)函数中\n",
    "```text \n",
    "1. 只接受字符串类型的参数，不能为DataFrame或者Series，如果数据存储为这两种，需要遍历后再作为参数传递。\n",
    "2. 返回值为生成器(generator)，对于生成器可以用循环直接迭代\n",
    "3. 生成器是迭代器的特殊形式，按需生成数据，而非一次生成所有数据，有利于降低内存占用，适合处理流式数据以及需要按行处理的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法：定义分词函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\persi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.797 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def word_cut(sr): # 此处text为字符串，而非DataFrame或Series\n",
    "    word_list = []\n",
    "    flag_set = ['n', 'vn', 'nz']\n",
    "    for string in sr:\n",
    "        row_word = []\n",
    "        for seg_word in psg.cut(string): # 循环迭代 psg.cut(text)产生的生成器\n",
    "            word = re.sub(r'[^\\u4e00-\\u9fa5]', '', seg_word.word)\n",
    "            if word and len(word) > 1 and seg_word.flag in flag_set and word not in stop_list:\n",
    "                row_word.append(word)\n",
    "        word_list.append((' ').join(row_word))\n",
    "    return word_list\n",
    "df['word_cutted'] = word_cut(df.text)\n",
    "\n",
    "# 保存分词结果，下次使用时直接读取\n",
    "file_save_path = Path().resolve().parent/'resource'/'data'/'processed'/f'{Path(file_path).stem}_cutted{Path(file_path).suffix}'\n",
    "df.to_csv(file_save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、训练LDA模型\n",
    "    使用 gensim 库中的 LdaModel 来训练一个主题模型\n",
    "## 1. 创建字典\n",
    "```text   \n",
    "因为 LDA 模型在内部处理时需要使用数字而不是文本数据，因此通过 corpora.Dictionary() 创建一个字典，这个字典会为每个唯一的词分配一个整数 ID。\n",
    "corpora.Dictionary()函数需要一个文档集合作为输出，其中每个文档都是词汇列表的形式。也即，该函数需要传递一个嵌套的列表作为输入，其中最内层的列表包含一段文本的分词结果。而在上面定义的word_cut()函数返回的值为字符串，因此需要现将字符串分割为列表，用到的方法为split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "df['word_cutted'] = df['word_cutted'].apply(lambda x: x.split())\n",
    "dictionary = corpora.Dictionary(df['word_cutted'])\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 建立词袋\n",
    "```text \n",
    "使用 dictionary.doc2bow() 方法为每个文档创建一个词袋（bag-of-words, BOW）。这个方法统计每个唯一词的出现次数，并将文本转换为一个稀疏向量。稀疏向量是指数据中为零的数据远多于非零数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in df.word_cutted]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练LDA模型\n",
    "`通过ldamodel.LdaModel()函数来训练LDA模型，其中包含7个主要的参数：`\n",
    "```text \n",
    "1. corpus：文档的词袋表示\n",
    "2. id2word：词典\n",
    "3. num_topics：主题的数量\n",
    "\n",
    "4. alpha：\n",
    "alpha 参数是与文档-主题分布相关的狄利克雷先验的参数。它决定了文档中主题的分布：\n",
    "高 alpha 值：意味着每个文档很可能包含多个主题，即每个文档中的主题分布比较均匀。这会导致文档包含更多的主题，每个文档的主题分布较为平滑。\n",
    "低 alpha 值：意味着每个文档倾向于由少数几个主题主导。这导致每个文档的主题分布较为集中，即文档中的主题数量较少。\n",
    "alpha 的选择取决于对文档主题多样性的假设。在不确定时，可以通过模型选择方法（如交叉验证）来确定最优的 alpha 值。（可以设置为 'auto' 以自动学习）\n",
    "\n",
    "5. eta：\n",
    "eta 参数是与主题-词分布相关的狄利克雷先验的参数。它决定了每个主题中词的分布：\n",
    "高 eta 值：每个主题包含的词汇更均匀分布。这意味着每个主题将具有较广泛的词汇。\n",
    "低 eta 值：每个主题被少数几个词主导。这会导致每个主题的词分布较为集中，即主题专注于较少的词汇。\n",
    "与 alpha 类似，eta 的设定应基于对主题词分布的假设，而实际值通常通过实验来确定。（可以设置为 'auto' 以自动学习）\n",
    "\n",
    "6. passes：代表语料库要被重复遍历的次数以进行模型训练。每遍历一次完整的语料库称为一遍“pass”。\n",
    "7. random_state：控制随机性的种子，可以使得在不同运行中复现结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import ldamodel\n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, alpha=0.1, eta=0.01, passes=100, random_state=2)\n",
    "lda.save(r'resource/result/lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 调用LDA模型并打印每个主题最重要的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lda_topics(lda_model, dictionary, n_top_words):\n",
    "    tword = []\n",
    "    for topic_idx in range(lda_model.num_topics): # 遍历所有主题\n",
    "        topic_word = []\n",
    "        topic_str = 'topic ' + str(topic_idx)\n",
    "        topic_word.append(topic_str)\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        topic_terms = lda_model.get_topic_terms(topic_idx, topn=n_top_words)\n",
    "        topic_w = \" \".join([dictionary[term_id] for term_id, _ in topic_terms])\n",
    "        topic_word.append(topic_w)\n",
    "        tword.append(topic_word)\n",
    "        print(topic_w)\n",
    "    return tword\n",
    "\n",
    "lda = ldamodel.LdaModel.load(r'./resource/result/lda.model')\n",
    "n_top_words = 30  # 每个主题打印多少词语\n",
    "topic_word = print_lda_topics(lda, dictionary, n_top_words)\n",
    "topic_word_table = pd.DataFrame(data=topic_word, columns=['主题', '主题前30个高频词'])\n",
    "topic_word_table.to_excel('topic_word_table.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 调用大语言模型构建主题标识类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "qianfan_config_path = 'resource/qianfan-config.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(qianfan_config_path, encoding='utf8')\n",
    "AK = config['qianfan']['ak']\n",
    "SK = config['qianfan']['sk']\n",
    "\n",
    "# 通过环境变量传递（作用于全局，优先级最低）\n",
    "import os\n",
    "os.environ[\"QIANFAN_ACCESS_KEY\"] = AK\n",
    "os.environ[\"QIANFAN_SECRET_KEY\"] = SK\n",
    "\n",
    "import qianfan\n",
    "row_data = pd.read_excel('topic_word_table.xlsx')\n",
    "data = row_data['主题前30个高频词']\n",
    "chat = qianfan.ChatCompletion()\n",
    "response = chat.do(model='ERNIE-Speed-128k', messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"请根据以下数据,为每一行提炼一个标题。要求如下：1. 每个标题的字数和格式必须一致。2. 标题不能包含“……主题”、“……专题”或“……高频词汇”。3. 每行的总结必须独立输出，不得合并或遗漏。4. 以JSON格式输出。5. 仅输出主题标识类别及其对应内容，无需额外解释。数据如下：{data}\"\n",
    "    }\n",
    "\n",
    "])\n",
    "\n",
    "print(response['body']['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`对于自定义的print_lda_topics函数：`\n",
    "`参数:`\n",
    "```text \n",
    "lda_model: 这是经过训练的 LDA 模型对象。\n",
    "dictionary: 这是一个 gensim 字典对象，用于将词的 ID 转换为词本身。\n",
    "n_top_words: 指定要显示的每个主题的关键词数量。\n",
    "```\n",
    "\n",
    "`流程:`\n",
    "\n",
    "```text \n",
    "1. 函数遍历模型中的每个主题。\n",
    "2. 对于每个主题，使用 get_topic_terms 方法提取前 n_top_words 个最重要的词。\n",
    "3. 将这些词的 ID 转换为词本身，并拼接成一个字符串。\n",
    "4. 打印并收集每个主题的关键词字符串。\n",
    "```\n",
    "\n",
    "`返回值:`\n",
    "\n",
    "```text \n",
    "返回一个列表，其中每个元素是一个代表主题的字符串，包含了该主题的前 n_top_words 个关键词。\n",
    "```\n",
    "`代码注解：`\n",
    "\n",
    "`1. tword = [ ]`\n",
    "\n",
    "```text \n",
    "初始化一个空列表，用来存储每个主题的关键词列表。最终，这个列表将包含每个主题的关键词字符串，每个字符串由该主题的前 n_top_words 个最重要的词组成。\n",
    "```\n",
    "`2. for topic_idx in range(lda_model.num_topics):`\n",
    "\n",
    "```text \n",
    "lda_model.num_topics 方法返回lda模型的主题数，得到一个数字。所以range(lda_model.num_topics) 本质上就是range(number) 。\n",
    "\n",
    "range(number)是range(start, end, sep)的缩写形式，此时end=number。因此，range(lda_model.num_topics) 生成一个从 0 到 lda_model.num_topics - 1 的整数序列，代表所有的主题编号。形式为[0,1,2,3……]\n",
    "\n",
    "整个for循环语句就用于循环遍历所有主题\n",
    "```\n",
    "`3. print(\"Topic #%d:\" % topic_idx)`\n",
    "\n",
    "```text \n",
    "打印：Topic #0：Topic #1：…… Topic #7:\n",
    "```\n",
    "\n",
    "`4. lda_model.get_topic_terms(topic_idx, topn=n_top_words)`\n",
    "\n",
    "```text \n",
    "获取当前主题的前 n_top_words 个最重要的词。这个方法返回一个列表，其中的元素是 (word_id, probability) 对，表示词的ID和该词在主题中的权重。\n",
    "```\n",
    "\n",
    "`5. \" \".join([dictionary[term_id] for term_id, _ in topic_terms])`\n",
    "```text \n",
    "[dictionary[term_id] for term_id, _ in topic_terms]\n",
    "\n",
    "这部分是一个列表推导式，用于从 topic_terms 列表中提取每个元素的 term_id，然后使用这个 term_id 从 dictionary 对象中查找相应的词。\n",
    "\n",
    "topic_terms 是一个列表，其中的每个元素是一个由 (term_id, probability) 组成的元组。这个元组表示在当前主题中，某个词（由 term_id 标识）的重要性（由 probability 表示）。\n",
    "\n",
    "term_id, _ 这部分代码使用 Python 的解构（unpacking）功能来提取元组中的 term_id。下划线 _ 在这里用作占位符，表示忽略第二个元素（probability）。\n",
    "\n",
    "dictionary[term_id]\n",
    "\n",
    "这是一个通过 term_id 从 dictionary 中获取对应词汇的操作。dictionary 是一个 gensim.corpora.Dictionary 对象，它存储了整个语料库中所有词的索引和映射。\n",
    "\n",
    "\" \".join(...):\n",
    "\n",
    "join 方法将一个字符串列表合并成一个单一的字符串，其中原来的列表元素通过指定的分隔符（这里是空格 \" \"）连接起来。\n",
    "这一步将上面列表推导产生的所有词合并为一个字符串，每个词之间用空格分隔。这样生成的字符串为当前主题的直观文本表达，列出了该主题的关键词。\n",
    "```\n",
    "`6. tword.append(topic_w)`\n",
    "```text \n",
    "将构建好的字符串（包含主题中的关键词）添加到列表 tword 中\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 可视化主题结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "def lda_visualization(lda, corpus, dictionary):\n",
    "    vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
    "    pyLDAvis.display(vis_data)\n",
    "    pyLDAvis.save_html(vis_data, 'lda_pass.html')\n",
    "lda_visualization(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型优化\n",
    "## 1. 结合TF-IDF优化词袋corpus\n",
    "```text \n",
    "TF-IDF可以帮助我们识别出语料库中最重要的词汇。这些词汇通常对文档的主题具有很高的判别力。通过这个步骤，我们可以减少需要考虑的词汇数量，从而降低模型的复杂性和计算成本。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用tfidf优化词袋\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus = tfidf[corpus]\n",
    "\n",
    "# 用优化后的词袋重新训练LDA模型\n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5,  alpha=0.1, eta=0.01,random_state=2,passes=100)\n",
    "\n",
    "# 调用优化后的模型展示主题并可视化\n",
    "n_top_words = 30  \n",
    "topic_word = print_lda_topics(lda, dictionary, n_top_words)\n",
    "lda_visualization(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 确定主题数量\n",
    "```text \n",
    "主题数量太少不能捕捉数据中所有的细节信息，但是主题数量太多又会增加模型的解释难度，因此主题数量的确定需要在模型的准确性和解释的复杂性之间寻求平衡。\n",
    "\n",
    "如何让主题数量在合理的范围从而控制解释的复杂性相对主观性强一些，可以灵活变通。但模型的准确性有相对客观的模型评价指标来进行量化。一般主要考虑两个指标：困惑度和主题一致性。\n",
    "```\n",
    "- `困惑度是衡量概率模型预测能力的一种指标，它的核心思想是，一个好的模型应该能很好地预测看不见的数据。`\n",
    "- `较低的困惑度意味着模型预测新数据的能力更强，在统计上更可能生成观测到的数据。`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def perplexity(topics_num_range):\n",
    "    perplexity_list =[]\n",
    "    for num_topics in topics_num_range:\n",
    "        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=100,alpha=0.1, eta=0.01,random_state=2)  \n",
    "        log_perplexity = lda_model.log_perplexity(corpus)\n",
    "        perplexity = np.exp2(-log_perplexity)\n",
    "        perplexity_list.append(perplexity)\n",
    "    plt.plot(topics_num_range, perplexity_list)\n",
    "    plt.xlabel('主题数目')\n",
    "    plt.ylabel('困惑度大小')\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    matplotlib.rcParams['axes.unicode_minus']=False\n",
    "    plt.title('主题-困惑度变化情况')\n",
    "    plt.show()\n",
    "\n",
    "perplexity(range(5,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 主题一致性\n",
    "- `主题一致性度量一个主题的词在语义上是否相关。`\n",
    "- `高一致性得分通常表明主题的词汇在语义上是相关的，这使得主题更容易被人理解和解释。`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一致性\n",
    "def coherence_lda(num_topics_range, corpus, texts, dictionary ):\n",
    "    coherence_dict =dict()\n",
    "    coherence_list =[]\n",
    "    for num_topics in num_topics_range:\n",
    "        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=100,alpha=0.1, eta=0.01,random_state=1)  \n",
    "        coherence = models.CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='u_mass').get_coherence()  # coherence 参数指定了计算一致性得分的具体方法。'u_mass' 是其中一种常用的一致性评价方法。\n",
    "        coherence_dict['num_topics'] = coherence\n",
    "        coherence_list.append(coherence)\n",
    "    plt.plot(num_topics_range,coherence_list)\n",
    "    plt.xlabel('主题数目')\n",
    "    plt.ylabel('coherence大小')\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    matplotlib.rcParams['axes.unicode_minus']=False\n",
    "    plt.title('主题-coherence变化情况')\n",
    "    plt.show()\n",
    "    return coherence_dict\n",
    "\n",
    "coherence_dict = coherence_lda(num_topics_range=range(2,6), corpus=corpus, texts=df.word_cutted, dictionary=dictionary)\n",
    "max_coherence_values = max(coherence_dict.\n",
    "                           values())\n",
    "max_coherence_key = [key for key, values in coherence_dict.items() if values == max_coherence_values]\n",
    "print(\n",
    "    f'主题{max_coherence_key}的coherence值最大')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, alpha=0.1, eta=0.01, passes=100, random_state=2)\n",
    "lda_visualization(best_lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_document_topics_and_max(lda_model, corpus):\n",
    "    # 初始化列表，用于存储每个文档的主题概率和最可能的主题\n",
    "    all_topics = []\n",
    "    most_likely_topics = []\n",
    "    most_likely_topics_prob = []\n",
    "    \n",
    "    # 遍历每个文档，获取主题分布\n",
    "    for doc_bow in corpus:\n",
    "        # 获取文档的所有主题及其概率，确保即使概率为0也能获取到所有主题\n",
    "        doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "        # 使用字典存储当前文档的所有主题和相应的概率\n",
    "        topic_probs = {f\"Topic_{int(topic_id)}\": prob for topic_id, prob in doc_topics}\n",
    "        \n",
    "        # 找出概率最大的主题和概率值\n",
    "        max_topic, max_prob = max(doc_topics, key=lambda item: item[1], default=(None, 0))\n",
    "        topic_probs['Most_Likely_Topic'] = int(max_topic) if max_topic is not None else None\n",
    "        topic_probs['Most_Likely_Topic_Prob'] = max_prob\n",
    "        \n",
    "        # 将主题概率字典添加到列表中\n",
    "        all_topics.append(topic_probs)\n",
    "        most_likely_topics.append(int(max_topic) if max_topic is not None else None)\n",
    "        most_likely_topics_prob.append(max_prob)\n",
    "    \n",
    "    return all_topics, most_likely_topics, most_likely_topics_prob\n",
    "\n",
    "# 假设 lda 和 corpus 已经定义\n",
    "all_doc_topics, most_likely_topics, most_likely_topics_prob = get_document_topics_and_max(lda, corpus)\n",
    "\n",
    "# 创建新的DataFrame来存储所有主题和概率\n",
    "topics_df = pd.DataFrame(all_doc_topics)\n",
    "\n",
    "# 合并原始DataFrame和新创建的topics DataFrame\n",
    "df = pd.concat([df, topics_df], axis=1)\n",
    "\n",
    "# 保存更新后的DataFrame到Excel文件\n",
    "df.to_excel(\"updated_document_topics.xlsx\", index=False)\n",
    "print(\"文档及其主题概率已成功更新并保存到 updated_document_topics.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topics(lda_model, corpus):\n",
    "    topics_distribution_list = []\n",
    "    for doc in corpus:\n",
    "        topics_distribution = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        topics_distribution_list.append(topics_distribution)\n",
    "    return topics_distribution_list\n",
    "topics_distribution_list = get_document_topics(lda, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info = []\n",
    "for doc_id, doc_dist in enumerate(topics_distribution_list):\n",
    "    doc_topics_info = [y for x,y in doc_dist]\n",
    "    max_doc_topics = max(doc_dist, key= lambda x: x[1])[0]\n",
    "    doc_topics_info.append(max_doc_topics)  \n",
    "    topics_info.append(doc_topics_info)\n",
    "topics_info\n",
    "\n",
    "data = pd.DataFrame(data=topics_info, columns=['#0','#1','#2','#3','#4','所属主题'])\n",
    "data.index.name = '文档编号'\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('topics_distribution.xlsx',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font_path = \"resource/SimHei.ttf\"\n",
    "\n",
    "\n",
    "def plot_topic_wordcloud(lda_model, dictionary, num_topics):\n",
    "    for topic_id in range(num_topics):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        topic_terms = lda_model.get_topic_terms(topic_id, topn=30)\n",
    "        topic_words = {dictionary[word_id]: prob for word_id, prob in topic_terms}\n",
    "        \n",
    "        # 指定 font_path 以支持中文显示\n",
    "        wordcloud = WordCloud(font_path=font_path, width=1200, height=600, background_color='white').generate_from_frequencies(topic_words)\n",
    "        \n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f'Topic #{topic_id + 1}')\n",
    "        plt.show()\n",
    "\n",
    "# 调用函数\n",
    "plot_topic_wordcloud(lda, dictionary, num_topics=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、计算主题时间强度变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算日期-主题强度\n",
    "def get_document_topics(lda_model, corpus):\n",
    "    doc_topics = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "    return doc_topics\n",
    "\n",
    "doc_topics = get_document_topics(lda, corpus)\n",
    "\n",
    "# 添加日期列并转换为日期时间类型\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 创建时间段列，按十年分段\n",
    "def get_decade(date):\n",
    "    year = date.year\n",
    "    # 5年\n",
    "    period_start = (year // 5) * 5\n",
    "    return f\"{period_start}-{period_start + 4}\"\n",
    "\n",
    "    # #10年 \n",
    "    # decade_start = (year // 10) * 10\n",
    "    # return f\"{decade_start}-{decade_start + 9}\"\n",
    "\n",
    "df['decade'] = df['date'].apply(get_decade)\n",
    "\n",
    "# 初始化主题强度数据结构\n",
    "topic_strength = {i: [] for i in range(lda.num_topics)}\n",
    "\n",
    "# 遍历文档，计算每篇文档的主题分布并按时间段聚合\n",
    "for doc_id, (decade, topic_dist) in enumerate(zip(df['decade'], doc_topics)):\n",
    "    for topic_id, prob in topic_dist:\n",
    "        topic_strength[topic_id].append((decade, prob))\n",
    "\n",
    "# 聚合每个时间段的主题强度\n",
    "aggregated_strength = {i: {} for i in range(lda.num_topics)}\n",
    "\n",
    "for topic_id, decade_probs in topic_strength.items():\n",
    "    DF = pd.DataFrame(decade_probs, columns=['decade', 'prob'])\n",
    "    df_grouped = DF.groupby('decade').mean()\n",
    "    aggregated_strength[topic_id] = df_grouped\n",
    "\n",
    "# 绘制每个主题的强度随时间段变化的折线图\n",
    "plt.figure(figsize=(14, 8))\n",
    "for topic_id, df_grouped in aggregated_strength.items():\n",
    "    plt.plot(df_grouped.index, df_grouped['prob'], label=f'Topic {topic_id}')\n",
    "\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Topic Strength')\n",
    "plt.title('Topic Strength Over Decades')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 计算日期-主题强度\n",
    "def get_document_topics(lda_model, corpus):\n",
    "    doc_topics = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "    return doc_topics\n",
    "\n",
    "doc_topics = get_document_topics(lda, corpus)\n",
    "\n",
    "# 添加日期列并转换为日期时间类型\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 创建时间段列，按五年分段\n",
    "def get_decade(date):\n",
    "    year = date.year\n",
    "    period_start = (year // 5) * 5\n",
    "    return f\"{period_start}-{period_start + 4}\"\n",
    "\n",
    "df['decade'] = df['date'].apply(get_decade)\n",
    "\n",
    "# 初始化主题强度数据结构\n",
    "topic_strength = {i: [] for i in range(lda.num_topics)}\n",
    "\n",
    "# 遍历文档，计算每篇文档的主题分布并按时间段聚合\n",
    "for doc_id, (decade, topic_dist) in enumerate(zip(df['decade'], doc_topics)):\n",
    "    for topic_id, prob in topic_dist:\n",
    "        topic_strength[topic_id].append((decade, prob))\n",
    "\n",
    "# 聚合每个时间段的主题强度\n",
    "aggregated_strength = {i: {} for i in range(lda.num_topics)}\n",
    "\n",
    "for topic_id, decade_probs in topic_strength.items():\n",
    "    df_topic = pd.DataFrame(decade_probs, columns=['decade', 'prob'])\n",
    "    \n",
    "    # 转换时间段为数值类型\n",
    "    df_topic['decade'] = df_topic['decade'].apply(lambda x: int(x.split('-')[0]))\n",
    "    \n",
    "    df_grouped = df_topic.groupby('decade').mean()\n",
    "    aggregated_strength[topic_id] = df_grouped\n",
    "\n",
    "# 创建3D图形\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 调整视角和背景\n",
    "ax.view_init(elev=20, azim=45)  # 调整视角以更好展示三维图形\n",
    "ax.set_facecolor('white')  # 设置背景为白色以提高可读性\n",
    "\n",
    "# 设置颜色映射和线条透明度\n",
    "colors = plt.cm.get_cmap('tab10', lda.num_topics)  # 使用colormap来分配不同的颜色\n",
    "alpha_value = 0.8  # 设置线条透明度\n",
    "\n",
    "# 绘制每个主题的三维折线图\n",
    "for topic_id, df_grouped in aggregated_strength.items():\n",
    "    x = df_grouped.index.values  # 时间段作为X轴\n",
    "    y = np.array([topic_id] * len(x))  # 主题ID作为Y轴\n",
    "    z = df_grouped['prob'].values  # 主题强度作为Z轴（高度）\n",
    "\n",
    "    # 确保所有数据都是数值类型，并过滤无效数据\n",
    "    valid_indices = ~np.isnan(x) & ~np.isnan(y) & ~np.isnan(z)\n",
    "    x = x[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "    z = z[valid_indices]\n",
    "\n",
    "    # 使用不同的颜色和透明度绘制线条\n",
    "    ax.plot(x, y, z, label=f'Topic {topic_id}', color=colors(topic_id), alpha=alpha_value)\n",
    "\n",
    "# 添加网格和标签\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Decade')\n",
    "ax.set_ylabel('Topic ID')\n",
    "ax.set_zlabel('Topic Strength')\n",
    "ax.set_title('Topic Strength Over Decades')\n",
    "\n",
    "# 添加图例并优化显示\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 计算日期-主题强度\n",
    "def get_document_topics(lda_model, corpus):\n",
    "    doc_topics = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "    return doc_topics\n",
    "\n",
    "doc_topics = get_document_topics(lda, corpus)\n",
    "\n",
    "# 添加日期列并转换为日期时间类型\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 创建时间段列，按五年分段\n",
    "def get_decade(date):\n",
    "    year = date.year\n",
    "    period_start = (year // 5) * 5\n",
    "    return f\"{period_start}-{period_start + 4}\"\n",
    "\n",
    "df['decade'] = df['date'].apply(get_decade)\n",
    "\n",
    "# 初始化主题强度数据结构\n",
    "topic_strength = {i: [] for i in range(lda.num_topics)}\n",
    "\n",
    "# 遍历文档，计算每篇文档的主题分布并按时间段聚合\n",
    "for doc_id, (decade, topic_dist) in enumerate(zip(df['decade'], doc_topics)):\n",
    "    for topic_id, prob in topic_dist:\n",
    "        topic_strength[topic_id].append((decade, prob))\n",
    "\n",
    "# 聚合每个时间段的主题强度\n",
    "aggregated_strength = {i: {} for i in range(lda.num_topics)}\n",
    "\n",
    "for topic_id, decade_probs in topic_strength.items():\n",
    "    df_topic = pd.DataFrame(decade_probs, columns=['decade', 'prob'])\n",
    "    \n",
    "    # 转换时间段为数值类型\n",
    "    df_topic['decade'] = df_topic['decade'].apply(lambda x: int(x.split('-')[0]))\n",
    "    \n",
    "    df_grouped = df_topic.groupby('decade').mean()\n",
    "    aggregated_strength[topic_id] = df_grouped\n",
    "\n",
    "# 创建3D图形\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 准备数据用于表面填充\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "z_vals = []\n",
    "\n",
    "for topic_id, df_grouped in aggregated_strength.items():\n",
    "    x = df_grouped.index.values  # 时间段作为X轴\n",
    "    y = np.array([topic_id] * len(x))  # 主题ID作为Y轴\n",
    "    z = df_grouped['prob'].values  # 主题强度作为Z轴（高度）\n",
    "\n",
    "    x_vals.append(x)\n",
    "    y_vals.append(y)\n",
    "    z_vals.append(z)\n",
    "\n",
    "# 将数据转换为二维矩阵，适合使用 plot_surface\n",
    "x_vals = np.array(x_vals)\n",
    "y_vals = np.array(y_vals)\n",
    "z_vals = np.array(z_vals)\n",
    "\n",
    "# 使用 plot_surface 进行填充\n",
    "surf = ax.plot_surface(x_vals, y_vals, z_vals, cmap='viridis', edgecolor='none', alpha=0.8)\n",
    "\n",
    "# 添加颜色条\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# 设置轴标签和标题\n",
    "ax.set_xlabel('Decade')\n",
    "ax.set_ylabel('Topic ID')\n",
    "ax.set_zlabel('Topic Strength')\n",
    "ax.set_title('Topic Strength Over Decades (Surface Plot)')\n",
    "\n",
    "# 调整视角\n",
    "ax.view_init(elev=30, azim=120)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 计算日期-主题强度\n",
    "def get_document_topics(lda_model, corpus):\n",
    "    doc_topics = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "    return doc_topics\n",
    "\n",
    "doc_topics = get_document_topics(lda, corpus)\n",
    "\n",
    "# 添加日期列并转换为日期时间类型\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 创建时间段列，按年份分段\n",
    "def get_year(date):\n",
    "    return date.year\n",
    "\n",
    "df['year'] = df['date'].apply(get_year)\n",
    "\n",
    "# 初始化主题强度数据结构\n",
    "topic_strength = {i: [] for i in range(lda.num_topics)}\n",
    "\n",
    "# 遍历文档，计算每篇文档的主题分布并按年份聚合\n",
    "for doc_id, (year, topic_dist) in enumerate(zip(df['year'], doc_topics)):\n",
    "    for topic_id, prob in topic_dist:\n",
    "        topic_strength[topic_id].append((year, prob))\n",
    "\n",
    "# 聚合每个年份的主题强度\n",
    "aggregated_strength = {i: {} for i in range(lda.num_topics)}\n",
    "\n",
    "for topic_id, year_probs in topic_strength.items():\n",
    "    df_topic = pd.DataFrame(year_probs, columns=['year', 'prob'])\n",
    "    df_grouped = df_topic.groupby('year').mean()\n",
    "    aggregated_strength[topic_id] = df_grouped\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# 设置颜色映射\n",
    "colors = plt.cm.get_cmap('tab10', lda.num_topics)\n",
    "\n",
    "# 绘制每个主题的折线图和填充区域\n",
    "for topic_id, df_grouped in aggregated_strength.items():\n",
    "    x = df_grouped.index.values  # 年份\n",
    "    y = df_grouped['prob'].values  # 主题强度\n",
    "\n",
    "    # 绘制折线图\n",
    "    plt.plot(x, y, color=colors(topic_id), label=f'Topic {topic_id}')\n",
    "\n",
    "    # 绘制填充区域\n",
    "    plt.fill_between(x, y, color=colors(topic_id), alpha=0.3)\n",
    "\n",
    "# 添加图例和标签\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Topic Strength')\n",
    "plt.title('Topic Strength Over Years (with Filled Areas)')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
